\documentclass[conference]{IEEEtran}
%Template version as of 6/27/2024
% \pdfoutput=1
\usepackage[T1]{fontenc}
\usepackage{cite}
\usepackage{url}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\graphicspath{{./images/}}
\usepackage{textcomp}
\usepackage{xcolor}
% \usepackage[UTF8]{ctex}
% \usepackage[english]{babel}
\usepackage{microtype}
\usepackage{multirow}
\usepackage{booktabs}   
\usepackage{adjustbox}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\newcommand{\todo}[1]{\textcolor{red}{TODO: #1}}
\IEEEoverridecommandlockouts

\begin{document}

\title{DualAttWaveNet: Multiscale Attention Networks for Satellite Interference Detection}

\author{
    \IEEEauthorblockN{
        Chunyu Yang,
        Boyu Yang,
        Kun Qiu,
        Zhe Chen,
        Yue Gao
    }
    \IEEEauthorblockA{
        School of Computer Science, Fudan University, China\\
        \{22307140114, 24110240144\}@m.fudan.edu.cn, \{qkun, zhechen, gao.yue\}@fudan.edu.cn
    }
    \thanks{This work was supported by the Fudan Undergraduate Research Opportunities Program (FDUROP) under Grant No.\,24198.}
}




\maketitle

\begin{abstract}
    The escalating overlap between non-geostationary orbit (NGSO) and geostationary orbit (GSO) satellite frequency allocations necessitates accurate interference detection methods that address two pivotal technical gaps: computationally efficient signal analysis for real-time operation, and robust anomaly discrimination under varying interference patterns.  Existing deep learning approaches employ encoder-decoder anomaly detectors that threshold input-output discrepancies for robustness. While the transformer-based TrID model achieves state-of-the-art performance (AUC: 0.8318, F1: 0.8321), its multi-head attention incurs huge computation time, and its decoupled training of time-frequency models overlooks cross-domain dependencies. To overcome these problems, we propose DualAttWaveNet. A bidirectional attention fusion layer dynamically correlates time-frequency samples using parameter-efficient cross-attention routing. A wavelet-regularized reconstruction loss enforces multi-scale consistency.  We train the model on public dataset which consists of 48 hours of satellite signals. Experiments show that compared to TrID, DualAttWaveNet improves AUC by 12\% and reduces inference time by 50\% to 540ms per batch while maintaining F1-score.
\end{abstract}

\begin{IEEEkeywords}
    interference detection, multimodal fusion, bidirectional attention, wavelet transform
\end{IEEEkeywords}


\section{Introduction}
\label{sec:intro}

The rapid increase of low Earth orbit (LEO) satellite systems presents significant challenges for next-generation communication networks. Industry projections indicate over 20,000 satellites will be deployed by leading operators, including SpaceX's Starlink \cite{starlink} and Starshield \cite{spacex_starshield}, as well as Eutelsat OneWeb \cite{oneweb}, by the end of the decade. These mega-constellations have become critical infrastructure for global connectivity, simultaneously commercializing space-based communications and expanding broadband access to previously underserved regions \cite{reddyLowEarthOrbit2023}. However, this exponential growth in satellite deployment creates fundamental technical obstacles-particularly the rising risk of spectrum overlap between LEO and geosynchronous orbit (GSO) satellites-necessitating scalable interference management frameworks that can evolve alongside expanding LEO networks.

Current research in satellite interference management generally follows three approaches. Preventive measures aim to reduce risks before system deployment \cite{sharmaInlineInterferenceMitigation2016, liOptimalBeamPower2019}, static mitigation methods are applied after interference occurs \cite{wangCoFrequencyInterferenceAnalysis2020, zhangSpectralCoexistenceLEO2018}, and simulation-based models predict interference using limited time or location samples. While useful in controlled settings, these methods face significant limitations in real-world environments \cite{yunDynamicDownlinkInterference2023}. Preventive measures rely on assumptions that often prove invalid after deployment, static mitigation strategies cannot adapt to dynamic interference patterns, and simulation models frequently fail to account for unexpected environmental factors such as solar radiation fluctuations or atmospheric variations \cite{facskoSpaceWeatherEffects2023}. Furthermore, traditional detection techniques that depend on fixed thresholds or static signal features cannot reliably identify complex, real-time interference. Addressing these shortcomings requires new detection solutions that achieve both rapid real-time response and consistent accuracy across diverse interference conditions.

Approaches to interference detection in satellite communications can be broadly categorized into traditional analytical methods and machine learning (ML)-based techniques. Conventional methods primarily employ energy detection (ED), which calculates signal energy over predefined intervals for threshold-based anomaly identification \cite{kay2009fundamentals}, or exploit spectral cyclostationary features to distinguish interference from periodic signals \cite{experimentalCyclostationary}. However, these approaches require manually calibrated thresholds and demonstrate limited sensitivity in low signal-to-noise ratio (SNR) environments. In contrast, ML-driven methods eliminate such constraints by automatically learning discriminative interference signatures from raw data. Classification-based solutions utilize deep neural networks to establish decision boundaries \cite{pellacoSpectrumPredictionInterference2019}, while encoder-decoder architectures treat detection as an anomaly identification task by reconstructing interference-free waveforms from corrupted inputs. Recent advancements have further deployed transformer models to capture long-range spectral dependencies, improving detection accuracy for persistent anomalies \cite{saifaldawlaGenAIBasedModelsNGSO2024}.

Despite these advancements, several critical challenges persist in existing interference detection methods. First, threshold-dependent traditional approaches-exemplified by energy detection-show degraded reliability in low-SNR regimes, where static thresholds fail to dynamically adapt to noise fluctuations or interference intensity variations, resulting in frequent false negatives under rapidly evolving orbital conditions \cite{saifaldawlaGenAIBasedModelsNGSO2024}. Second, while attention-driven architectures achieve state-of-the-art detection sensitivity, their computational overhead from multi-head attention mechanisms creates significant latency during both training and inference, making them unsuitable for resource-constrained satellite edge devices. Third, contemporary deep learning models often process time-domain and frequency-domain signal representations in isolation by training separate networks for each modality. This decoupled approach yields suboptimal performance-time-domain models achieve an AUC of only 0.8318 while frequency-domain models score a mere 0.7106, fundamentally limiting detection capability.

\begin{figure}[tbp]
    \centering
    \includegraphics[width=\linewidth]{system-model.pdf}
    \caption{Interference scenario between GSO and LEO satellite systems.}
    \label{fig:interference-scenario}
\end{figure}

To address these challenges, we propose \textbf{DualAttWaveNet}, a unified model that integrates cross-domain signal fusion for real-time interference detection. Our primary contributions are threefold:

\begin{enumerate}
    \item The proposed architecture jointly processes time-domain samples and their frequency-domain representations through a bidirectional attention mechanism. This design eliminates the need for explicit multi-head computation while enabling adaptive correlation learning between spectral and temporal features.
    \item To enhance robustness against minor signal fluctuations, we introduce a wavelet-constrained reconstruction loss. This is achieved by applying discrete wavelet transform (DWT) decomposition to both raw and reconstructed signals, enforcing cross-band consistency through multi-scale subspace constraints.
    \item Comprehensive evaluation on a public satellite communication dataset (48 hours duration) demonstrates DualAttWaveNet's superiority: 12\% higher AUC and 50\% faster inference compared to state-of-the-art baselines, while maintaining competitive F1-score.
\end{enumerate}

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{overview.pdf}
    \caption{Summary of our approach. Architecture of the cross-modal signal enhancement model showing dual-branch processing: time domain (800d) and frequency domain (800d) inputs are downsampled to 64d, fused through bidirectional attention into a 128d latent space ($\mathcal{Z}$), then upsampled to produce a reconstructed 1600d signal. Wavelet transforms ($X_\text{wave}$ and $\hat{X}_{\text{wave}}$) are applied to the original and reconstructed signals.}

    \label{fig:overview}
\end{figure*}

The remainder of this paper is organized as follows. Section 2 provides background information and motivations. Section 3 details the architecture of DualAttWaveNet. Section 4 presents our experimental results and analysis. Finally, Section 5 concludes the paper with a summary of our findings and directions for future work.

\section{Background}
\label{sec:background}

\subsection{Interference Scenario}



Satellite communication systems using the same frequency band often interfere with each other. As shown in Fig.~\ref{fig:interference-scenario}, we focus on two types of satellites. GSO Satellite flies in a fixed position 36,000 km above Earth. It serves as the main signal source for a ground station (GGS). LEO Satellites move rapidly at 500-2,000 km altitudes. Their signals can interfere with GSO signals due to spectrum overlap. The composite received waveform of GGS contains both desired carrier signals and interference.

The received carrier power $C$ from a geosynchronous orbit (GSO) satellite is calculated based on classical satellite link budget principles:
\begin{equation}
    C = \frac{\text{EIRP}_{\text{gso}} \cdot G_{\text{r, gso}}(\theta_0)}{L_{\text{FS, gso}} \cdot L_{\text{add}}}
    \label{eq:carrier_power}
\end{equation}
where $\text{EIRP}$ is GSO satellite equivalent isotropic radiated power, $G_{\text{r, gso}}(\theta_0)$ denotes the maximum receive antenna gain at boresight angle $\theta_0$, $L_{\text{FS, gso}}$ represents free-space path loss, and $L_{\text{add}}$ accounts for aggregate atmospheric impairments.

Interference from $K$ low Earth orbit (LEO) satellites is modeled as the superposition of individual interference terms:
\begin{equation}
    I_k = \frac{\text{EIRP}_k \cdot G_{\text{r, k}}(\theta_k) \cdot B_{\text{adj, k}}}{L_{\text{FS, k}} \cdot L_{\text{add}}}
    \label{eq:interference_power}
\end{equation}

The angular gain term $G_{\text{r, k}}(\theta_k)$ reflects spatial relationships caused by LEO orbital motion, while $B_{\text{adj, k}} \in [0,1]$ is the spectral overlap between GSO and LEO transmissions.

The signal received by physical layer at the GGS has three components:
\begin{align}
    y(t) =\, & x(t)\sqrt{\text{CNR}}\, (\text{Desired GSO}) \nonumber                                                 \\[0.5em]
             & + \sum_{k=1}^{K} I_k(t)e^{j2\pi \Delta f_k t}\sqrt{\text{INR}_k}\, (\text{LEO interference}) \nonumber \\[0.5em]
             & + \zeta(t)\, (\text{Thermal noise})
\end{align}
where $x(t)$  is the desired GSO signal, $\Delta f_k = f_{\text{c},k} - f_{\text{c,gso}}$ captures carrier frequency offsets from Doppler effects. The exponential terms induce time-varying phase rotations proportional to relative satellite motion. Here, $\text{CNR}$ (Carrier-to-Noise Ratio) and $\text{INR}_k$ (Interference-to-Noise Ratio) respectively characterize the desired signal quality and interference intensity relative to the noise floor.

Dual signal representations are derived for machine learning processing:
\begin{itemize}
    \item Time-domain: $y^A$ captures instantaneous amplitude variations through uniform sampling
    \item Frequency-domain: Welch's power spectral density estimation generates logarithmic magnitude spectra via overlapping windowed transforms \cite{saifaldawlaGenAIBasedModelsNGSO2024}: $y^F = 10\log_{10}(\phi(y(t)))$
\end{itemize}



\subsection{Deep Learning Approaches for Interference Detection}
\label{ssec:related_works}

Recent advances in machine learning have significantly shifted the landscape of satellite interference detection, moving away from traditional signal processing techniques toward anomaly-based approaches. One widely adopted method uses encoder-decoder frameworks that are trained to reconstruct clean, interference-free waveforms from raw signal inputs. The core idea is that interference introduces deviations from the expected signal, which can be identified by comparing input and output waveforms.

Unlike regression-based models that struggle to capture the diversity of interference types, reconstruction-based methods are more flexible, as they directly highlight anomalies through mismatches between original and reconstructed signals. Early work by Pellaco et al. \cite{pellacoSpectrumPredictionInterference2019} introduced an LSTM autoencoder (LSTMAE) for identifying interference in non-geostationary satellite data. However, the sequential nature of LSTM limits parallelism, making the model less suitable for real-time systems. To address this, Saifaldawla et al. \cite{saifaldawlaConvolutionalAutoencodersNonGeostationary2024} proposed convolutional autoencoders (CAE) that process time domain signal directly.

More recently, transformer-based architectures have been adopted to capture long-range dependencies in signal data. Notably, Saifaldawla et al. \cite{saifaldawlaGenAIBasedModelsNGSO2024} proposed the Transformer-based Interference Detector (TrID), which leverages self-attention mechanisms to identify global interference patterns that traditional convolutional or recurrent layers may miss. In their implementation, TrID encodes each input—consisting of 800 sequential time-domain samples—into a 64-dimensional latent vector using stacked transformer layers. The decoder subsequently reconstructs the waveform through learned upsampling operations. During the training phase, a mean squared error (MSE) loss between the original and reconstructed signal guides the model to learn interference-free patterns. At inference time, samples containing interference produce larger reconstruction errors, which TrID flags as anomalies when they exceed a predetermined threshold.

Despite their performance, transformer-based models come with drawbacks. Their computational complexity scales quadratically with input length, making them difficult to apply in wideband satellite systems. Moreover, many current approaches treat time and frequency information separately, missing out on the potential benefits of jointly modeling these domains. This limitation reduces the ability to learn cross-domain features that could improve robustness under diverse interference conditions.

\section{Overall Design}
\label{sec:model}


\subsection{Proposed Deep Learning Model}
\label{subsec:proposed_model}

To address the computational cost of multi-head attention and improve classification accuracy, we propose DualAttWaveNet-an autoencoder that processes dual-domain inputs and reconstructs both representations at the same time.

As shown in \figurename~\ref{fig:overview}, the architecture employs separate encoders and decoders for time and frequency domains. Our key innovation is the bidirectional attention fusion module (Section~\ref{subsec:bi_attn}), which replaces multi-head attention with single-head dot-product attention and spatial reduction. This design minimizes computational overhead while capturing cross-domain dependencies, with fusion occurring before decoding to enhance reconstruction accuracy and detection performance.

Both domain inputs have shape $B \times 800$ (batch size $B$). Dedicated convolution modules downsample these to $B \times 64$ latent representations, which undergo bidirectional attention processing. The resulting cross-modal features are concatenated and fed to transposed convolution layers for upsampling, reconstructing outputs of shape $B \times 1600$. The model is trained using a composite loss combining MSE with wavelet-domain regularization (Section~\ref{subsec:wavelet}), enforcing multi-scale consistency to preserve both temporal and spectral characteristics.

\subsection{Bidirectional Attention}
\label{subsec:bi_attn}

We introduce a parameter-efficient mutual attention mechanism for cross-modal feature fusion. Unlike traditional multi-head attention \cite{vaswaniAttentionAllYou2017}, our method uses single-head dot-product attention with spatial reduction to lower computational costs while preserving inter-domain alignment, as shown in \figurename~\ref{fig:bidirectional-attention}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.65\linewidth]{bidirectional-attention.pdf}
    \caption{Bidirectional attention mechanism in DualAttWaveNet. Both attention phases share the same parameters $Q$, $K$, and $V$, implemented as 1D convolutions to reduce channel dimensions.}
    \label{fig:bidirectional-attention}
\end{figure}

For input feature maps $\mathbf{X} \in \mathbb{R}^{B \times C \times L}$ (time domain) and $\mathbf{Y} \in \mathbb{R}^{B \times C \times L}$ (frequency domain), the mutual attention operator is defined as:
\begin{equation}
    \begin{aligned}
        \text{AttentionGate}(\mathbf{X}, \mathbf{Y}) & = \mathbf{V}_y \cdot \text{Softmax}\Bigl(\frac{\mathbf{Q}_x\, \mathbf{K}_y^\top}{\sqrt{d}}\Bigr) \\
        \text{MutualAttn}(\mathbf{X}, \mathbf{Y})    & = \mathbf{X} + \gamma \cdot \text{AttentionGate}(\mathbf{X}, \mathbf{Y})
    \end{aligned}
\end{equation}
where $\gamma$ is a learnable scalar (initialized to 0) and $\mathbf{Q}_x = \mathcal{W}_Q(\mathbf{X}) \in \mathbb{R}^{B \times L \times \frac{C}{8}}$, $\mathbf{K}_y = \mathcal{W}_K(\mathbf{Y}) \in \mathbb{R}^{B \times \frac{C}{8} \times L}$, and $\mathbf{V}_y = \mathcal{W}_V(\mathbf{Y}) \in \mathbb{R}^{B \times C \times L}$. Here, $\mathcal{W}_{Q,K,V}$ are 1D convolutional layers with kernel size 1 that reduce channel dimensions by a factor of 8.

The reduced representations are multiplied to obtain an $L \times L$ affinity matrix, capturing position-wise correlations between the two modalities. A row-wise softmax is then applied to normalize the scores.

The same attention mechanism is applied symmetrically in both directions:
\begin{equation}
    \begin{aligned}
        \widehat{\mathbf{X}} & = \text{MutualAttn}(\mathbf{X}, \mathbf{Y})\,,           \\
        \widehat{\mathbf{Y}} & = \text{MutualAttn}(\mathbf{Y}, \widehat{\mathbf{X}})\,.
    \end{aligned}
\end{equation}

Our bidirectional attention significantly reduces computational complexity compared to standard multi-head attention. For feature maps with $C$ channels and length $L$, multi-head attention with $h$ heads requires $\mathcal{O}(hCL^2)$ operations for computing attention weights. In contrast, our single-head design with 8× channel reduction requires only $\mathcal{O}(\frac{C}{8}L^2)$ operations. Additionally, while multi-head attention needs separate projection matrices for each head (totaling $3hC^2$ parameters), our approach uses only $3 \times \frac{C^2}{8}$ parameters shared across both attention directions. Importantly, we treat both modalities $\mathbf{X}$ and $\mathbf{Y}$ as equally important by allowing each to serve as both query and key in the attention computation. This symmetric design ensures fair representation learning, as neither modality is privileged over the other-unlike asymmetric attention where one modality only provides keys while the other provides queries. 



\subsection{Wavelet-Domain Spectral Regularization}
\label{subsec:wavelet}

Standard MSE loss alone does not fully capture a signal's complex structure. To address this, we introduce a wavelet loss that enforces reconstruction consistency across multiple scales. For an input tensor $\mathbf{x}\in\mathbb{R}^{B\times C\times L}$ (with batch size $B$, channels $C$, and length $L$), we build a learnable filter bank $\mathcal{F}\in\mathbb{R}^{S\times 1\times K}$ parameterized by
\begin{equation}
    \label{eq:wavelet}
    \mathcal{W}_s = \operatorname{Norm}\Bigl(\cos\Bigl(\frac{\tau}{s}\Bigr) \odot \mathcal{G}(\tau,s)\Bigr),
\end{equation}
where $\mathcal{G}(\tau,s)=\exp\left(-\frac{\tau^2}{2s^2}\right)$, $s\in\mathbb{S}$ are the scale parameters, $K=4s_{\text{max}}$ sets the kernel size, and $\tau$ is the index within $-2K$ and $2K$. Each filter is $L_2$-normalized to maintain energy consistency.

We implement the discrete wavelet transform as a depth-wise 1D convolution:
\begin{equation}
    \begin{aligned}
        \mathbf{X}_{\text{wave}} & = \text{Conv1D}(\mathbf{X}, \mathcal{F})                                                            \\
                                 & = \bigcup_{s\in\mathbb{S}} \mathbf{X}\ast \mathcal{W}_s \in \mathbb{R}^{B\times C\times S\times L}.
    \end{aligned}
\end{equation}
Reflection padding is used to mitigate boundary artifacts while preserving temporal resolution. \figurename~\ref{fig:wavelet-transform} illustrates example wavelet kernels and their filtering effects.

\begin{figure}[tbp]
    \centering
    \includegraphics[width=\linewidth]{wavelet-transform.pdf}
    \caption{Multi-scale wavelet analysis using Morlet wavelets. (Top-left) Normalized wavelet kernels at scales $s \in \{5, 10, 20\}$, with increasing temporal support for larger scales. (Top-right) Input signal. (Bottom) Wavelet transform responses at different scales demonstrating the scale-frequency trade-off: (Bottom-left) Fine scale ($s=5$, red) captures high-frequency details and transients; (Bottom-right) Coarse scale ($s=20$, green) emphasizes low-frequency components with increased smoothing. The dashed line shows the original input for reference.}
    \label{fig:wavelet-transform}
\end{figure}

Our overall loss function combines the standard reconstruction loss with a wavelet-domain regularization term:
\begin{equation}
    \mathcal{L} = \|\mathbf{\hat{X}} - \mathbf{X}\|_2^2 + 0.5\sum_{s\in\mathbb{S}} \|\mathbf{\hat{X}}_{\text{wave}}^{(s)} - \mathbf{X}_{\text{wave}}^{(s)}\|_2^2,
\end{equation}
where the first term measures the mean squared error (MSE) between the reconstructed signal $\mathbf{\hat{X}}$ and the ground truth $\mathbf{X}$, while the second term, weighted by 0.5, enforces consistency in the wavelet domain across multiple scales $s \in \mathbb{S}$. This dual-domain optimization strategy ensures that the reconstruction preserves both temporal dynamics and multi-scale spectral characteristics of the original signal.

The model outputs a reconstructed signal, which is compared with the original input to detect interference. If the reconstruction error exceeds a threshold $\Gamma_{\text{th}} = \mu + \text{std}(\mathbf{L})$, where $\mu$ and $\text{std}(\mathbf{L})$ are the mean and standard deviation of validation losses, the signal is classified as containing interference. This approach assumes normally distributed validation losses, with outliers (losses exceeding one standard deviation above the mean) indicating interference.



\section{Experiments}
\label{sec:experiments}

\subsection{Data Configuration}

The synthetic dataset was generated via a 48-hour MATLAB simulation, sampling Ku-band (10.7-12.7 GHz) interference scenarios every 10 seconds, producing 17,281 temporal snapshots as described in \cite{saifaldawlaGenAIBasedModelsNGSO2024}. Each instance includes synchronized time-domain and frequency-domain representations: an 800-point waveform captures signal amplitudes, and an 800-bin spectral magnitude is obtained through FFT processing.

Binary classification labels are determined by link budget analysis, where class 0 represents non-interference scenarios ($\text{INR} < \Gamma_{\text{th}}$) and class 1 denotes substantial interference ($\text{INR} \geq \Gamma_{\text{th}}$). Input signals in both domains are normalized separately to zero mean and unit variance.

The dataset is partitioned with anomaly detection in mind. The training (11,509 samples) and validation (1,302 samples) sets consist exclusively of non-interference data (class 0), as the reconstruction-based models are trained to minimize reconstruction error on clean signals, while the test set is balanced with 2,235 samples each of class 0 and class 1. The simulation incorporates time-varying link losses (0-9 dB), extreme interference cases with peak aggregate INR of 32.47 dB, and background CNR fluctuations between 6.40 and 15.40 dB. All experiments were conducted on a laptop equipped with an RTX 3050Ti GPU (4GB VRAM).



\subsection{Baseline Models}

We compare our method against several state-of-the-art reconstruction-based models: \textbf{CNNAE} utilizing 1D convolutional layers for the encoder and an MLP for the decoder; \textbf{CNNAE+Attention} that augments CNNAE with standard temporal self-attention modules (not bidirectional); and domain-specific \textbf{TrID}  embedding spectral correlation priors, evaluated in two variants: \textbf{TrID (Signal)} and \textbf{TrID (Spectrum)} with results directly obtained from the original paper using the same dataset \cite{saifaldawlaGenAIBasedModelsNGSO2024}. Additionally, we include \textbf{Transformer AE} which adapts TrID's architecture to dual-input processing (both signal and spectrum). While TrID processes a single input modality, all other baseline methods (CNNAE, CNNAE+Attention, and Transformer AE) utilize both signal and spectrum inputs. All baselines follow the standard autoencoder paradigm where inputs are reconstructed from bottleneck embeddings, with identical training protocols for fair comparison.

\subsection{Evaluation Results}

As shown in Table \ref{tab:main_results}, DualAttWaveNet achieves state-of-the-art performance across all key metrics. It records the highest AUC score (0.9327), surpassing the TrID(Signal) baseline by 10.09\% and showing a 25.15\% absolute improvement over the Transformer AE (0.6812). Moreover, our framework completes inference in 0.5409s-53.26\% of TrID (Signal)'s 1.0156s-while maintaining comparable F1 scores with both the task-specialized TrID (Signal) (0.8321) and lightweight models like CNNAE (0.8054). Notably, the F1 score of DualAttWaveNet (0.8351) remains on par with TrID (Signal), effectively balancing precision and recall through integrated dual-attention mechanisms.


\begin{table}[tbp]
    \caption{Performance Comparison of DualAttWaveNet Against Baseline Models}
    \label{tab:main_results}
    \centering
    \resizebox{\linewidth}{!}{
        \begin{tabular}{lcccc}
            \toprule
            \textbf{Model}  & \textbf{Accuracy (\%) } $\uparrow$ & \textbf{F1 Score} $\uparrow$ & \textbf{AUC}$\uparrow$ & \textbf{Time(s)} $\downarrow$ \\
            \midrule
            DualAttWaveNet  & 0.8351                             & 0.8351                       & 0.9327                 & 0.5409                        \\
            \cmidrule{1-5}
            % LinearAE        & 0.8149                             & 0.8149                       & 0.9176                 & 0.0966                        \\
            CNNAE           & 0.8020                             & 0.8054                       & 0.8825                 & 0.1654                        \\
            CNNAE+Attention & 0.7695                             & 0.7691                       & 0.8719                 & 0.5969                        \\
            TrID (Signal)            & 0.8318                             & 0.8321                       & 0.8318                 & 1.0156                        \\
            TrID (Spectrum)            & 0.7112                        & 0.6838                      & 0.7106                 & 1.0143                        \\
            Transformer AE  & 0.6812                             & 0.5921                       & 0.6812                 & 1.8354                        \\
            \bottomrule
        \end{tabular}}
\end{table}

\begin{figure}[tbp]
    \centering
    \includegraphics[width=0.8\linewidth]{roc-comparison.pdf}
    \caption{ROC curves for DualAttWaveNet and baseline models. DualAttWaveNet achieves the highest AUC score.}
    \label{fig:roc_comparison}
\end{figure}

\begin{figure*}
    \centering
    \includegraphics[width=0.88\linewidth]{confusion_new.pdf}
    \caption{Confusion matrix for DualAttWaveNet and other baseline models.}
    \label{fig:confusion_matrix}
\end{figure*}

Baseline analyses reveal key architectural limitations: the Transformer AE's stacked multi-head attention layers incur substantial latency (1.8354s) and suffer from overparameterization, resulting in significant performance degradation (-24.30\% F1 compared to our method). We evaluate TrID in both signal and spectrum domains, with TrID (Spectrum) showing significantly degraded performance (0.6838 F1) compared to TrID (Signal) (0.8321 F1). Although domain-specific TrID (Signal) achieves near-parity accuracy (0.8318), its AUC score of 0.8318 indicates an overfitting to spectral correlation priors. Similarly, the temporal attention in CNNAE+Attention increases computation time by 3.6× over vanilla CNNAE (0.5969s vs. 0.1654s) while degrading AUC by 1.06\%, highlighting the inefficiency of ill-configured attention modules compared to our optimized dual-attention design. As visualized in \figurename~\ref{fig:roc_comparison}, the ROC curves clearly illustrate the discriminative power among competing models. DualAttWaveNet's left-skewed curve (AUC = 0.9327) dominates the upper-left quadrant, achieving an 83.5\% true positive rate at less than 10\% false positives. 


\subsection{Ablation Study}

As shown in Table \ref{tab:ablation}, the full DualAttWaveNet achieves the best performance with 83.51\% accuracy, 83.51\% F1 score, and 0.9327 AUC. Removing the mutual attention mechanism ("w/o Mutual Attention") causes consistent performance drops. Disabling the wavelet loss ("w/o Wavelet Loss") further degrades accuracy by 0.8\%, proving the necessity of joint time-frequency optimization. The vanilla implementation exhibits significant performance limitations (79.95\% accuracy, 0.9175 AUC), demonstrating a 3.6\% accuracy gap compared to the full model, which highlights the collective contribution of our dual-attention design and spectral-aware constraints to robust classification capability.

\begin{table}[tbp]
    \caption{Ablation Study of DualAttWaveNet Components}
    \label{tab:ablation}
    \centering

    \begin{tabular}{lccc}
        \toprule
        \textbf{Model Variant} & \textbf{Accuracy (\%)} $\uparrow$ & \textbf{F1 Score} $\uparrow$ & \textbf{AUC} $\uparrow$ \\
        \midrule
        DualAttWaveNet (Full)  & 0.8351                            & 0.8351                       & 0.9327                  \\
        \cmidrule{1-4}
        w/o Mutual Attention   & 0.8289                            & 0.8288                       & 0.9294                  \\
        w/o Wavelet Loss       & 0.8273                            & 0.8273                       & 0.9283                  \\
        Vanilla Implementation & 0.7995                            & 0.7975                       & 0.9175                  \\
        \bottomrule
    \end{tabular}

\end{table}

\section{Conclusion}
\label{sec:conclusion}

In this paper, we present DualAttWaveNet, a computationally-efficient multimodal framework for interference detection in GSO/LEO coexistence systems. We used a bidirectional attention mechanism to fuse time and frequency domain signals. This design integrates the capability of attention module and at the same time uses less computational resources. Additionally, we introduced a wavelet loss besides traditional MSE loss to enforce the reconstructed output to be consistent across all scales. Extensive evaluations in synthetic Ku-band scenarios show DualAttWaveNet achieves 0.9327 AUC with 83.51\% accuracy, surpassing state-of-the-art baselines TrID. The model is trained and tested on an edge NVIDIA 3050Ti GPU, demonstrating 50\% faster inference time compared to TrID while maintaining competitive F1 score. Future work will optimize spectral downsampling strategies for different spectrum overlaps scenarios.

\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}